{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from data_loader import load_dataset\n",
    "from model_trainer import train\n",
    "\n",
    "PATH_TRAIN = \"../Data/Processed/Train.nc\"\n",
    "PATH_TEST = \"../Data/Processed/Test.nc\"\n",
    "PATH_VAL = \"../Data/Processed/Val.nc\"\n",
    "\n",
    "PATH_WEIGHTS_BEST = \"../Models/PINN-Best.pth\"\n",
    "PATH_WEIGHTS_CURRENT = \"../Models/PINN-Current.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataDrivenModule(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels=768, out_channels=768, kernel_size=5, padding=1, \n",
    "                 embed_dim=768, num_heads=4, \n",
    "                 dropout_p=0.1, \n",
    "                 mlp_hidden_dim=768,  \n",
    "                 transconv_out_channels=15):\n",
    "        super(DataDrivenModule, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, padding=padding)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_hidden_dim, mlp_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_hidden_dim, out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.transconv = nn.ConvTranspose2d(in_channels=out_channels, out_channels=transconv_out_channels, kernel_size=kernel_size, padding=padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv(x)\n",
    "\n",
    "        batch_size, channels, height, width = x.shape\n",
    "        x = x.view(batch_size, channels, -1).permute(2, 0, 1)  # Flatten and reshape for attention\n",
    "\n",
    "        x, _ = self.attn(x, x, x)  # Apply attention\n",
    "        x = self.norm(x)  # Apply normalization\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "        x = self.mlp(x)  # Apply MLP\n",
    "\n",
    "        x = x.permute(1, 2, 0).view(batch_size, channels, height, width)  # Reshape back\n",
    "\n",
    "        x = self.transconv(x)  # Transposed Conv2D\n",
    "\n",
    "        return x  # Returns u', v', SSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicsInformedModule(nn.Module):\n",
    "    def __init__(self, g=9.81, f=1e-4):\n",
    "        super(PhysicsInformedModule, self).__init__()\n",
    "        self.g = g\n",
    "        self.f = f\n",
    "\n",
    "    def forward(self, ssh):\n",
    "        # Compute gradients ∂SSH/∂x and ∂SSH/∂y using finite differences\n",
    "        dudx = torch.diff(ssh, dim=-1, append=ssh[:, :, :, -1:])\n",
    "        dvdy = torch.diff(ssh, dim=-2, append=ssh[:, :, -1:, :])\n",
    "        \n",
    "        # Compute geostrophic velocity components\n",
    "        u_g = self.g / self.f * dvdy\n",
    "        v_g = -self.g / self.f * dudx\n",
    "        return u_g, v_g\n",
    "    \n",
    "class SumModule(nn.Module):\n",
    "    def forward(self, u_g, v_g, u_prime, v_prime):\n",
    "        u = u_g + u_prime\n",
    "        v = v_g + v_prime\n",
    "        return u, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PICPModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PICPModel, self).__init__()\n",
    "        self.data_module = DataDrivenModule()\n",
    "        self.physics_module = PhysicsInformedModule()\n",
    "        self.sum_module = SumModule()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Step 1: Data-driven module\n",
    "        output = self.data_module(x)  # Produces u', v', SSH\n",
    "        u_prime, v_prime, ssh = torch.chunk(output, chunks=3, dim=1)\n",
    "\n",
    "        # Step 2: Physics-informed module\n",
    "        u_g, v_g = self.physics_module(ssh)\n",
    "\n",
    "        # Step 3: Sum module\n",
    "        u, v = self.sum_module(u_g, v_g, u_prime, v_prime)\n",
    "\n",
    "        return torch.stack((u, v, ssh), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedLoss(nn.Module):\n",
    "    def __init__(self, rho=2.0):\n",
    "        super(WeightedLoss, self).__init__()\n",
    "        self.rho = rho\n",
    "        self.mse = nn.MSELoss(reduction='none')  # Compute loss for each point\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        # Compute SSC magnitude\n",
    "        ssc = torch.sqrt(targets[:, 0, :, :]**2 + targets[:, 1, :, :]**2)  # sqrt(u^2 + v^2)\n",
    "\n",
    "        # Compute 85th percentile threshold (U_max15%)\n",
    "        U0 = torch.quantile(ssc, 0.85)\n",
    "\n",
    "        # Create weight matrix (rho for high SSC, 1 otherwise)\n",
    "        weight = torch.ones_like(ssc)\n",
    "        weight[ssc > U0] = self.rho\n",
    "\n",
    "        # Compute weighted MSE loss\n",
    "        loss = self.mse(predictions, targets)\n",
    "        loss = weight.unsqueeze(1) * loss  # Apply weights\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "kernel_size = 5\n",
    "num_heads = 4\n",
    "rho = 5.0\n",
    "\n",
    "model = PICPModel(kernel_size=kernel_size, num_heads=num_heads)\n",
    "loss_function = WeightedLoss(rho=rho)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_ds = load_dataset(PATH_TRAIN, batch_size=batch_size)\n",
    "val_ds = load_dataset(PATH_VAL, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model = model,\n",
    "    loss_function = loss_function, \n",
    "    optimizer = optimizer,\n",
    "    train = train_ds, \n",
    "    val = val_ds,\n",
    "    epochs = epochs, \n",
    "    path_weights_best = PATH_WEIGHTS_BEST, \n",
    "    path_weights_last = PATH_WEIGHTS_CURRENT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
