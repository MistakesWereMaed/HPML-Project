{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from data_loader import load_dataset\n",
    "\n",
    "PATH_TRAIN = \"../Data/Processed/Train.nc\"\n",
    "PATH_TEST = \"../Data/Processed/Test.nc\"\n",
    "PATH_VAL = \"../Data/Processed/Val.nc\"\n",
    "\n",
    "PATH_WEIGHTS = \"PINN-Best.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataDrivenModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DataDrivenModule, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels=768, out_channels=768, kernel_size=3, padding=1)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=768, num_heads=8)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.norm = nn.LayerNorm(768)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(768, 768),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(768, 768),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(768, 768)\n",
    "        )\n",
    "        self.transconv = nn.ConvTranspose2d(in_channels=768, out_channels=45, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        b, c, h, w = x.shape\n",
    "        x = x.view(b, c, -1).permute(2, 0, 1)  # Flatten and reshape for attention\n",
    "        x, _ = self.attn(x, x, x)  # Apply attention\n",
    "        x = self.norm(x)  # Apply normalization\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "        x = F.relu(self.mlp(x))  # Apply MLP\n",
    "        x = x.permute(1, 2, 0).view(b, c, h, w)  # Reshape back\n",
    "        x = self.transconv(x)  # Transposed Conv2D\n",
    "        return x  # Returns u', v', SSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicsInformedModule(nn.Module):\n",
    "    def __init__(self, g=9.81, f=1e-4):\n",
    "        super(PhysicsInformedModule, self).__init__()\n",
    "        self.g = g\n",
    "        self.f = f\n",
    "\n",
    "    def forward(self, ssh):\n",
    "        # Compute gradients ∂SSH/∂x and ∂SSH/∂y using finite differences\n",
    "        dudx = torch.diff(ssh, dim=-1, append=ssh[:, :, :, -1:])\n",
    "        dvdy = torch.diff(ssh, dim=-2, append=ssh[:, :, -1:, :])\n",
    "        \n",
    "        # Compute geostrophic velocity components\n",
    "        u_g = self.g / self.f * dvdy\n",
    "        v_g = -self.g / self.f * dudx\n",
    "        return u_g, v_g\n",
    "    \n",
    "class SumModule(nn.Module):\n",
    "    def forward(self, u_g, v_g, u_prime, v_prime):\n",
    "        u = u_g + u_prime\n",
    "        v = v_g + v_prime\n",
    "        return u, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PICPModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PICPModel, self).__init__()\n",
    "        self.data_module = DataDrivenModule()\n",
    "        self.physics_module = PhysicsInformedModule()\n",
    "        self.sum_module = SumModule()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Step 1: Data-driven module\n",
    "        output = self.data_module(x)  # Produces u', v', SSH\n",
    "        u_prime, v_prime, ssh = torch.chunk(output, chunks=3, dim=1)\n",
    "\n",
    "        # Step 2: Physics-informed module\n",
    "        u_g, v_g = self.physics_module(ssh)\n",
    "\n",
    "        # Step 3: Sum module\n",
    "        u, v = self.sum_module(u_g, v_g, u_prime, v_prime)\n",
    "\n",
    "        return u, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedLoss(nn.Module):\n",
    "    def __init__(self, rho=2.0):\n",
    "        super(WeightedLoss, self).__init__()\n",
    "        self.rho = rho\n",
    "        self.mse = nn.MSELoss(reduction='none')  # Compute loss for each point\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        # Compute SSC magnitude\n",
    "        ssc = torch.sqrt(targets[:, 0, :, :]**2 + targets[:, 1, :, :]**2)  # sqrt(u^2 + v^2)\n",
    "\n",
    "        # Compute 85th percentile threshold (U_max15%)\n",
    "        U0 = torch.quantile(ssc, 0.85)\n",
    "\n",
    "        # Create weight matrix (rho for high SSC, 1 otherwise)\n",
    "        weight = torch.ones_like(ssc)\n",
    "        weight[ssc > U0] = self.rho\n",
    "\n",
    "        # Compute weighted MSE loss\n",
    "        loss = self.mse(predictions, targets)\n",
    "        loss = weight.unsqueeze(1) * loss  # Apply weights\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, best_val_loss, filename=PATH_WEIGHTS):\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"best_val_loss\": best_val_loss\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"Model checkpoint saved at epoch {epoch+1} with val_loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, optimizer, filepath=PATH_WEIGHTS):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    start_epoch = checkpoint[\"epoch\"] + 1\n",
    "    loss = checkpoint[\"loss\"]\n",
    "\n",
    "    print(f\"Resuming training from epoch {start_epoch}\")\n",
    "    return start_epoch, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, loss_function, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            predictions = model(inputs)\n",
    "            predictions = predictions.view(-1, 2, 30, 100)\n",
    "            \n",
    "            loss = loss_function(predictions, targets)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "    return avg_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, train, val, loss_function, optimizer, num_epochs, start_epoch=0):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for inputs, targets in train:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            predictions = model(inputs)\n",
    "            predictions = predictions.view(-1, 2, 30, 100)\n",
    "\n",
    "            loss = loss_function(predictions, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "        val_loss = validate(model, val, loss_function, device)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            save_checkpoint(model, optimizer, epoch, best_val_loss)\n",
    "\n",
    "    print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load_dataset(PATH_TRAIN)\n",
    "val = load_dataset(PATH_VAL)\n",
    "\n",
    "model = PICPModel()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_function = WeightedLoss(rho=5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "try:\n",
    "    start_epoch, prev_loss = load_checkpoint(model, optimizer)\n",
    "except FileNotFoundError:\n",
    "    print(\"No checkpoint found, starting from scratch.\")\n",
    "    start_epoch = 0\n",
    "\n",
    "train(model, train, val, loss_function, optimizer, num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
